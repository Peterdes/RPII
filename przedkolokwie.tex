\input{include.tex}

\markright{Piotr Suwara\hfill Rachunek prawdopodobie≈Ñstwa II: Przedkolokwie \hfill}

\begin{document}
	\begin{proposition}[1.1]
		$(\delta_{x_n} \implies \delta_x) \iff x_n \to x$
	\end{proposition}
	
	\begin{proposition}[1.2]
		$\frac{1}{n} \sum_1^n \delta_{k/n} \implies \lambda$,
		$\lambda$ Lebesgue measure on $[0,1]$.
	\end{proposition}
	
	\begin{proposition}[1.3]
		If $X_n \to X$ a.s., then $X_n \implies X$.
		
		If $X_n \xrightarrow{\P} X$, then $X_n \implies X$.
		
		If $X_n \implies c$, then $X_n \xrightarrow{\P} c$.
	\end{proposition}
	
	\begin{proposition}[1.7]
		If $n p_n \to \lambda$, 
		then $\mr{Bin}(p_n, n) \implies \mr{Poiss}(\lambda)$.
	\end{proposition}
	
	\begin{proposition}[1.12]
		$\N(a_n, \sigma_n^2) \implies \N(a, \sigma^2)$ 
		iff $a_n \to a$ and $\sigma_n^2 \to \sigma^2$.
	\end{proposition}
	
	\begin{proposition}[2.1]
		Let $X$ have density and $a_n, a \geq 0$, 
		then $a_n X + b_n \implies aX+b$ iff $a_n \to a$ and $b_n \to b$.
	\end{proposition}
	
	\begin{proposition}[2.2]
		If $f$ continuous, $X_n \implies X$, then $f(X_n) \implies f(X)$.
	\end{proposition}
	
	\begin{theorem}[Scheffe; problem 2.4]
		If $g_n \to g$ a.s., then $\mu_n \implies \mu$.
	\end{theorem}
	
	\begin{proposition}[2.7]
		$X_n \implies X$, $X$ has continuous distribution, 
		then $F_{X_n} \rightrightarrows F_X$.
	\end{proposition}
	
	\begin{proposition}[3.4]
		Convex combinations of characteristic functions 
		are characteristic functions.
	\end{proposition}
	
	\begin{proposition}[3.7]
		If $\varphi_X$ has second derivative in $0$, then $\E X^2 < \infty$.
	\end{proposition}
	
	\begin{proposition}[5.9]
		If $X_n \implies X, Y_n \implies c$, 
		then $X_n + Y_n \implies c + X$ and $X_n Y_n \implies cX$.
	\end{proposition}
	
	\begin{proposition}[6.5]
		$X_1, X_2, \ldots$ iid,
		$\E X_1 = 0$, $\var X_1 = 1$, 
		$(a_n)$ bounded sequence,
		$s_n = \sqrt{a_1^2 + \ldots + a_n^2} \to \infty$, 
		then $s_n^{-1}(a_1 X_1 + \ldots + a_n X_n) \implies \N(0,1)$.
	\end{proposition}
	
	\begin{proposition}[6.7]
		$X \sim \N(a, B)$ has density iff $B$ is nondegenerate
		and then $\displaystyle g_X(x)
		= \frac{\sqrt{\det C}}{(2 \pi)^{d/2}}
		\exp \left( \frac{ \langle B^{-1}(x-a), x-a\rangle}{2} \right)$.
	\end{proposition}
	
	\begin{proposition}[7.6]
		$X, Y$ independent, $f$ Borel function, $\E |f(X,Y)| < \infty$,
		then $\E(f(X,Y) | Y) = g(Y)$ a.s. where $g(y) = \E(f(X, y))$.
	\end{proposition}
	
	\begin{proposition}[7.7]
		$X, Y$ integrable iid,
		then $\E(X|X+Y) = \E(Y|X+Y) = \frac{1}{2} (X+Y)$.
	\end{proposition}
	
	\begin{proposition}[8.1]
		$\tau, \sigma$ are stopping times for $(\F_n)$.
		Then $\tau + \sigma$ is too.
		Notice $\sigma=1$ is a~stopping time, so $\tau+1$ is too.
		Notice $\tau - 1$ in not a~stopping time in general.
	\end{proposition}
	
	\begin{proposition}[8.2]
		Let $X_n$ be $(\F_n)$-adapted, $B$ a~borel set.
		Then $\tau_1 = \inf\{n:X_n \in B\}$ is a~stopping time.
		And $\tau_k = \inf\{n>\tau_{k-1}: X_n \in B\}$ is also.
	\end{proposition}
	
	\begin{proposition}[8.3]
		$\tau, \sigma$ stopping times with respect to $(\F_n)$,
		then $\{\tau < \sigma\}, \{\tau \leq \sigma\}, \{\tau = \sigma\}
			\in \F_\tau \cap \F_\sigma$
		and $\F_\tau \cap \F_\sigma = \F_{\tau \wedge \sigma}$.
	\end{proposition}
	
	\begin{proposition}[8.5]
		$X_1, X_2, \ldots$ iid, $\E X_1 = 0, \var X_1 < \infty$,
		$S_n = X_1 + \ldots + X_n$.
		Then $S_n$, $S_n^2 - \var(S_n)$ are martingales with respect
		to the~filtration generated by $(X_n)$.
	\end{proposition}
	
	\begin{proposition}[9.1]
		Let $X_n$ be a~martingale.
		
		Then $|X_n|^p$ for $p \geq 1$ is a~submartingale.
		
		And $X_n \wedge a$ is a~supermartingale.
		
		And $X_n \vee a$ is a~submartingale.
	\end{proposition}

	
	\begin{proposition}[normal distribution]
		$X \sim \N(\mu, \sigma^2)$
		
		$$\E X = \mu 
		\qquad \var X = \sigma^2 
		\qquad g(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{(x-\mu)^2}{2 \sigma^2}} 
		\qquad \varphi(t) = e^{i\mu t - \frac{1}{2} \sigma^2 t^2}$$
	\end{proposition}
	
	\begin{proposition}[binomial distribution]
		$X \sim \mr{Bin}(n,p)$
		
		$$\E X = pn
		\qquad \var X = np(1-p)
		\qquad g(x) = \binom{n}{k} p^k (1-p)^{n-k}
		\qquad \varphi(t) = (1-p+pe^{it})^n$$
	\end{proposition}

	
	\begin{proposition}[geometric distribution -- 1st success]
		$X \sim \mr{Geom}(p)$
		
		$$\E X = \frac{1}{p}
		\qquad \var X = \frac{1-p}{p^2}
		\qquad g(x) = (1-p)^{k-1} p
		\qquad \varphi(t) = \frac{p e^{it}}{1 - (1-p) e^{it}}$$
	\end{proposition}
	
	\begin{proposition}[Poisson distribution]
		$X \sim \mr{Poiss}(\lambda)$
		
		$$\E X = \lambda
		\qquad \var X = \lambda
		\qquad g(x) = \frac{\lambda^k}{k!} e^{-\lambda}
		\qquad \varphi(t) = e^{\lambda (e^{it} - 1)}$$
	\end{proposition}
	
	\begin{proposition}[exponential distribution]
		$X \sim \mr{Exp}(\lambda)$
		
		$$\E X = \frac{1}{\lambda}
		\qquad \var X = \frac{1}{\lambda^2}
		\qquad g(x) = \lambda e^{-\lambda x}
		\qquad \varphi(t) = \left(1 - \frac{it}{\lambda}\right)^{-1} = \frac{\lambda}{\lambda - it}$$
	\end{proposition}
	
	\begin{proposition}[Cauchy distribution]
		$X \sim \mr{Cauchy}(h)$
		
		Not integrable!
		
		$$g(x) = \frac{h}{\pi(h^2 + x^2)}
		\qquad \varphi(t) = e^{-h|t|}$$
	\end{proposition}





\end{document}
 
 
